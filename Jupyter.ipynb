{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/masini/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /home/masini/anaconda3/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in /home/masini/anaconda3/lib/python3.8/site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in /home/masini/anaconda3/lib/python3.8/site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: click in /home/masini/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fake_news = pd.read_csv(\"data/Fake.csv\", index_col=0)\n",
    "true_news = pd.read_csv(\"data/True.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>As U.S. budget fight looms, Republicans flip their fiscal script</th>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U.S. military to accept transgender recruits on Monday: Pentagon</th>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Senior U.S. Republican senator: 'Let Mr. Mueller do his job'</th>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FBI Russia probe helped by Australian diplomat tip-off: NYT</th>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump wants Postal Service to charge 'much more' for Amazon shipments</th>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "title                                                                                                   \n",
       "As U.S. budget fight looms, Republicans flip th...  WASHINGTON (Reuters) - The head of a conservat...   \n",
       "U.S. military to accept transgender recruits on...  WASHINGTON (Reuters) - Transgender people will...   \n",
       "Senior U.S. Republican senator: 'Let Mr. Muelle...  WASHINGTON (Reuters) - The special counsel inv...   \n",
       "FBI Russia probe helped by Australian diplomat ...  WASHINGTON (Reuters) - Trump campaign adviser ...   \n",
       "Trump wants Postal Service to charge 'much more...  SEATTLE/WASHINGTON (Reuters) - President Donal...   \n",
       "\n",
       "                                                         subject  \\\n",
       "title                                                              \n",
       "As U.S. budget fight looms, Republicans flip th...  politicsNews   \n",
       "U.S. military to accept transgender recruits on...  politicsNews   \n",
       "Senior U.S. Republican senator: 'Let Mr. Muelle...  politicsNews   \n",
       "FBI Russia probe helped by Australian diplomat ...  politicsNews   \n",
       "Trump wants Postal Service to charge 'much more...  politicsNews   \n",
       "\n",
       "                                                                  date  \n",
       "title                                                                   \n",
       "As U.S. budget fight looms, Republicans flip th...  December 31, 2017   \n",
       "U.S. military to accept transgender recruits on...  December 29, 2017   \n",
       "Senior U.S. Republican senator: 'Let Mr. Muelle...  December 31, 2017   \n",
       "FBI Russia probe helped by Australian diplomat ...  December 30, 2017   \n",
       "Trump wants Postal Service to charge 'much more...  December 29, 2017   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/masini/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/masini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-543248ed2b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \"\"\"\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \"\"\"\n\u001b[1;32m   1356\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(fake_news['text'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['donald',\n",
       " 'trump',\n",
       " 'just',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'wish',\n",
       " 'all',\n",
       " 'americans',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'new',\n",
       " 'year',\n",
       " 'and',\n",
       " 'leave',\n",
       " 'it',\n",
       " 'at',\n",
       " 'that',\n",
       " 'instead',\n",
       " 'he',\n",
       " 'had',\n",
       " 'to',\n",
       " 'give',\n",
       " 'a',\n",
       " 'shout',\n",
       " 'out',\n",
       " 'to',\n",
       " 'his',\n",
       " 'enemies',\n",
       " 'haters',\n",
       " 'and',\n",
       " 'the',\n",
       " 'very',\n",
       " 'dishonest',\n",
       " 'fake',\n",
       " 'news',\n",
       " 'media',\n",
       " 'the',\n",
       " 'former',\n",
       " 'reality',\n",
       " 'show',\n",
       " 'star',\n",
       " 'had',\n",
       " 'just',\n",
       " 'one',\n",
       " 'job',\n",
       " 'to',\n",
       " 'do',\n",
       " 'and',\n",
       " 'he',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'do',\n",
       " 'it',\n",
       " 'as',\n",
       " 'our',\n",
       " 'country',\n",
       " 'rapidly',\n",
       " 'grows',\n",
       " 'stronger',\n",
       " 'and',\n",
       " 'smarter',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'wish',\n",
       " 'all',\n",
       " 'of',\n",
       " 'my',\n",
       " 'friends',\n",
       " 'supporters',\n",
       " 'enemies',\n",
       " 'haters',\n",
       " 'and',\n",
       " 'even',\n",
       " 'the',\n",
       " 'very',\n",
       " 'dishonest',\n",
       " 'fake',\n",
       " 'news',\n",
       " 'media',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'and',\n",
       " 'healthy',\n",
       " 'new',\n",
       " 'year',\n",
       " 'president',\n",
       " 'angry',\n",
       " 'pants',\n",
       " 'tweeted',\n",
       " 'will',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'year',\n",
       " 'for',\n",
       " 'america',\n",
       " 'as',\n",
       " 'our',\n",
       " 'country',\n",
       " 'rapidly',\n",
       " 'grows',\n",
       " 'stronger',\n",
       " 'and',\n",
       " 'smarter',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'wish',\n",
       " 'all',\n",
       " 'of',\n",
       " 'my',\n",
       " 'friends',\n",
       " 'supporters',\n",
       " 'enemies',\n",
       " 'haters',\n",
       " 'and',\n",
       " 'even',\n",
       " 'the',\n",
       " 'very',\n",
       " 'dishonest',\n",
       " 'fake',\n",
       " 'news',\n",
       " 'media',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'and',\n",
       " 'healthy',\n",
       " 'new',\n",
       " 'year',\n",
       " 'will',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'year',\n",
       " 'for',\n",
       " 'america',\n",
       " 'donald',\n",
       " 'j',\n",
       " 'trump',\n",
       " 'realdonaldtrump',\n",
       " 'december',\n",
       " 's',\n",
       " 'tweet',\n",
       " 'went',\n",
       " 'down',\n",
       " 'about',\n",
       " 'as',\n",
       " 'welll',\n",
       " 'as',\n",
       " 'you',\n",
       " 'd',\n",
       " 'expect',\n",
       " 'what',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'president',\n",
       " 'sends',\n",
       " 'a',\n",
       " 'new',\n",
       " 'year',\n",
       " 's',\n",
       " 'greeting',\n",
       " 'like',\n",
       " 'this',\n",
       " 'despicable',\n",
       " 'petty',\n",
       " 'infantile',\n",
       " 'gibberish',\n",
       " 'only',\n",
       " 'trump',\n",
       " 'his',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'decency',\n",
       " 'won',\n",
       " 't',\n",
       " 'even',\n",
       " 'allow',\n",
       " 'him',\n",
       " 'to',\n",
       " 'rise',\n",
       " 'above',\n",
       " 'the',\n",
       " 'gutter',\n",
       " 'long',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'wish',\n",
       " 'the',\n",
       " 'american',\n",
       " 'citizens',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'new',\n",
       " 'year',\n",
       " 'bishop',\n",
       " 'talbert',\n",
       " 'swan',\n",
       " 'talbertswan',\n",
       " 'december',\n",
       " 'one',\n",
       " 'likes',\n",
       " 'you',\n",
       " 'calvin',\n",
       " 'calvinstowell',\n",
       " 'december',\n",
       " 'impeachment',\n",
       " 'would',\n",
       " 'make',\n",
       " 'a',\n",
       " 'great',\n",
       " 'year',\n",
       " 'for',\n",
       " 'america',\n",
       " 'but',\n",
       " 'i',\n",
       " 'll',\n",
       " 'also',\n",
       " 'accept',\n",
       " 'regaining',\n",
       " 'control',\n",
       " 'of',\n",
       " 'congress',\n",
       " 'miranda',\n",
       " 'yaver',\n",
       " 'mirandayaver',\n",
       " 'december',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'yourself',\n",
       " 'talk',\n",
       " 'when',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'include',\n",
       " 'that',\n",
       " 'many',\n",
       " 'people',\n",
       " 'that',\n",
       " 'hate',\n",
       " 'you',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'wonder',\n",
       " 'why',\n",
       " 'do',\n",
       " 'the',\n",
       " 'they',\n",
       " 'all',\n",
       " 'hate',\n",
       " 'me',\n",
       " 'alan',\n",
       " 'sandoval',\n",
       " 'december',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'word',\n",
       " 'haters',\n",
       " 'in',\n",
       " 'a',\n",
       " 'new',\n",
       " 'years',\n",
       " 'wish',\n",
       " 'marlene',\n",
       " 'december',\n",
       " 'can',\n",
       " 't',\n",
       " 'just',\n",
       " 'say',\n",
       " 'happy',\n",
       " 'new',\n",
       " 'year',\n",
       " 'koren',\n",
       " 'pollitt',\n",
       " 'korencarpenter',\n",
       " 'december',\n",
       " 's',\n",
       " 'trump',\n",
       " 's',\n",
       " 'new',\n",
       " 'year',\n",
       " 's',\n",
       " 'eve',\n",
       " 'tweet',\n",
       " 'from',\n",
       " 'happy',\n",
       " 'new',\n",
       " 'year',\n",
       " 'to',\n",
       " 'all',\n",
       " 'including',\n",
       " 'to',\n",
       " 'my',\n",
       " 'many',\n",
       " 'enemies',\n",
       " 'and',\n",
       " 'those',\n",
       " 'who',\n",
       " 'have',\n",
       " 'fought',\n",
       " 'me',\n",
       " 'and',\n",
       " 'lost',\n",
       " 'so',\n",
       " 'badly',\n",
       " 'they',\n",
       " 'just',\n",
       " 'don',\n",
       " 't',\n",
       " 'know',\n",
       " 'what',\n",
       " 'to',\n",
       " 'do',\n",
       " 'love',\n",
       " 'donald',\n",
       " 'j',\n",
       " 'trump',\n",
       " 'realdonaldtrump',\n",
       " 'december',\n",
       " 'is',\n",
       " 'nothing',\n",
       " 'new',\n",
       " 'for',\n",
       " 'trump',\n",
       " 'he',\n",
       " 's',\n",
       " 'been',\n",
       " 'doing',\n",
       " 'this',\n",
       " 'for',\n",
       " 'years',\n",
       " 'trump',\n",
       " 'has',\n",
       " 'directed',\n",
       " 'messages',\n",
       " 'to',\n",
       " 'his',\n",
       " 'enemies',\n",
       " 'and',\n",
       " 'haters',\n",
       " 'for',\n",
       " 'new',\n",
       " 'year',\n",
       " 's',\n",
       " 'easter',\n",
       " 'thanksgiving',\n",
       " 'and',\n",
       " 'the',\n",
       " 'anniversary',\n",
       " 'of',\n",
       " 'pic',\n",
       " 'twitter',\n",
       " 'com',\n",
       " 'daniel',\n",
       " 'dale',\n",
       " 'december',\n",
       " 's',\n",
       " 'holiday',\n",
       " 'tweets',\n",
       " 'are',\n",
       " 'clearly',\n",
       " 'not',\n",
       " 'presidential',\n",
       " 'how',\n",
       " 'long',\n",
       " 'did',\n",
       " 'he',\n",
       " 'work',\n",
       " 'at',\n",
       " 'hallmark',\n",
       " 'before',\n",
       " 'becoming',\n",
       " 'president',\n",
       " 'steven',\n",
       " 'goodine',\n",
       " 'sgoodine',\n",
       " 'december',\n",
       " 's',\n",
       " 'always',\n",
       " 'been',\n",
       " 'like',\n",
       " 'this',\n",
       " 'the',\n",
       " 'only',\n",
       " 'difference',\n",
       " 'is',\n",
       " 'that',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'few',\n",
       " 'years',\n",
       " 'his',\n",
       " 'filter',\n",
       " 'has',\n",
       " 'been',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'roy',\n",
       " 'schulze',\n",
       " 'thbthttt',\n",
       " 'december',\n",
       " 'apart',\n",
       " 'from',\n",
       " 'a',\n",
       " 'teenager',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'term',\n",
       " 'haters',\n",
       " 'wendy',\n",
       " 'wendywhistles',\n",
       " 'december',\n",
       " 's',\n",
       " 'a',\n",
       " 'fucking',\n",
       " 'year',\n",
       " 'old',\n",
       " 'who',\n",
       " 'knows',\n",
       " 'december',\n",
       " 'to',\n",
       " 'all',\n",
       " 'the',\n",
       " 'people',\n",
       " 'who',\n",
       " 'voted',\n",
       " 'for',\n",
       " 'this',\n",
       " 'a',\n",
       " 'hole',\n",
       " 'thinking',\n",
       " 'he',\n",
       " 'would',\n",
       " 'change',\n",
       " 'once',\n",
       " 'he',\n",
       " 'got',\n",
       " 'into',\n",
       " 'power',\n",
       " 'you',\n",
       " 'were',\n",
       " 'wrong',\n",
       " 'year',\n",
       " 'old',\n",
       " 'men',\n",
       " 'don',\n",
       " 't',\n",
       " 'change',\n",
       " 'and',\n",
       " 'now',\n",
       " 'he',\n",
       " 's',\n",
       " 'a',\n",
       " 'year',\n",
       " 'older',\n",
       " 'photo',\n",
       " 'by',\n",
       " 'andrew',\n",
       " 'burton',\n",
       " 'getty',\n",
       " 'images']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "letras = re.findall(r'\\b[A-zÀ-úü]+\\b', exemplo.lower())\n",
    "letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops #lista de stopwords em inglês "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'donald trump wish americans happy new year leave instead give shout enemies haters dishonest fake news media former reality show star one job country rapidly grows stronger smarter want wish friends supporters enemies haters even dishonest fake news media happy healthy new year president angry pants tweeted great year america country rapidly grows stronger smarter want wish friends supporters enemies haters even dishonest fake news media happy healthy new year great year america donald j trump realdonaldtrump december tweet went welll expect kind president sends new year greeting like despicable petty infantile gibberish trump lack decency even allow rise gutter long enough wish american citizens happy new year bishop talbert swan talbertswan december one likes calvin calvinstowell december impeachment would make great year america also accept regaining control congress miranda yaver mirandayaver december hear talk include many people hate wonder hate alan sandoval december uses word haters new years wish marlene december say happy new year koren pollitt korencarpenter december trump new year eve tweet happy new year including many enemies fought lost badly know love donald j trump realdonaldtrump december nothing new trump years trump directed messages enemies haters new year easter thanksgiving anniversary pic twitter com daniel dale december holiday tweets clearly presidential long work hallmark becoming president steven goodine sgoodine december always like difference last years filter breaking roy schulze thbthttt december apart teenager uses term haters wendy wendywhistles december fucking year old knows december people voted hole thinking would change got power wrong year old men change year older photo andrew burton getty images'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_stopwords = [palavra for palavra in letras if palavra not in stops]\n",
    "palavras_importantes = \" \".join(sem_stopwords)\n",
    "palavras_importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 1.2 MB/s eta 0:00:01     |███████████████████████████     | 10.2 MB 1.2 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/masini/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/masini/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/masini/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/masini/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=69fc2a71b9cc9815c1f156d1823f03344d6e21a1c8298dac5a5510cb9b7417a0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bsls3w1k/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-2.3.2-cp38-cp38-manylinux1_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 642 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy) (4.47.0)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp38-cp38-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy) (1.18.5)\n",
      "Collecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp38-cp38-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.4-cp38-cp38-manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.3-cp38-cp38-manylinux2014_x86_64.whl (296 kB)\n",
      "\u001b[K     |████████████████████████████████| 296 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/masini/anaconda3/lib/python3.8/site-packages (from spacy) (49.2.0.post20200714)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.4-cp38-cp38-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.4-cp38-cp38-manylinux2014_x86_64.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/masini/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/masini/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/masini/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/masini/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Installing collected packages: wasabi, blis, plac, catalogue, cymem, murmurhash, preshed, srsly, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.4 murmurhash-1.0.4 plac-1.1.3 preshed-3.0.4 spacy-2.3.2 srsly-1.0.3 thinc-7.4.1 wasabi-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spc = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump wish americans happy new year leave instead give shout enemies haters dishonest fake news media former reality show star one job country rapidly grow stronger smarter want wish friends supporters enemies haters even dishonest fake news media happy healthy new year president angry pants tweet great year america country rapidly grow stronger smarter want wish friends supporters enemies haters even dishonest fake news media happy healthy new year great year america donald j trump realdonaldtrump december tweet go welll expect kind president send new year greeting like despicable petty infantile gibberish trump lack decency even allow rise gutter long enough wish american citizens happy new year bishop talbert swan talbertswan december one like calvin calvinstowell december impeachment would make great year america also accept regain control congress miranda yaver mirandayaver december hear talk include many people hate wonder hate alan sandoval december use word haters new years wish marlene december say happy new year koren pollitt korencarpenter december trump new year eve tweet happy new year include many enemies fight lose badly know love donald j trump realdonaldtrump december nothing new trump years trump direct messages enemies haters new year easter thanksgiving anniversary pic twitter com daniel dale december holiday tweets clearly presidential long work hallmark become president steven goodine sgoodine december always like difference last years filter break roy schulze thbthttt december apart teenager use term haters wendy wendywhistle december fucking year old know december people vote hole thinking would change get power wrong year old men change year older photo andrew burton getty images\n"
     ]
    }
   ],
   "source": [
    "spc_letras = spc(palavras_importantes)\n",
    "lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
    "texto_limpo = \" \".join(lemmas)\n",
    "print(texto_limpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    lower = texto.lower()\n",
    "    letras = re.findall(r'\\b[A-zÀ-úü]+\\b', lower)\n",
    "    stops = set(stopwords.words('english')) \n",
    "    palavras_sem_stopwords = [w for w in letras if w not in stops]\n",
    "    palavras_importantes = \" \".join(palavras_sem_stopwords) \n",
    "    spc_letras = spc(palavras_importantes)\n",
    "    lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
    "    # Junte os lemmas \n",
    "    texto_limpo = \" \".join(lemmas)\n",
    "    \n",
    "    return texto_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news['conteúdo noticia Limpo'] = fake_news['text'].apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>conteúdo noticia Limpo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing</th>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>donald trump wish americans happy new year lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drunk Bragging Trump Staffer Started Russian Collusion Investigation</th>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sheriff David Clarke Becomes An Internet Joke For Threatening To Poke People ‘In The Eye’</th>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>friday reveal former milwaukee sheriff david c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump Is So Obsessed He Even Has Obama’s Name Coded Into His Website (IMAGES)</th>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>christmas day donald trump announce would back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pope Francis Just Called Out Donald Trump During His Christmas Speech</th>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>pope francis use annual christmas day message ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "title                                                                                                   \n",
       " Donald Trump Sends Out Embarrassing New Year’s...  Donald Trump just couldn t wish all Americans ...   \n",
       " Drunk Bragging Trump Staffer Started Russian C...  House Intelligence Committee Chairman Devin Nu...   \n",
       " Sheriff David Clarke Becomes An Internet Joke ...  On Friday, it was revealed that former Milwauk...   \n",
       " Trump Is So Obsessed He Even Has Obama’s Name ...  On Christmas day, Donald Trump announced that ...   \n",
       " Pope Francis Just Called Out Donald Trump Duri...  Pope Francis used his annual Christmas Day mes...   \n",
       "\n",
       "                                                   subject               date  \\\n",
       "title                                                                           \n",
       " Donald Trump Sends Out Embarrassing New Year’s...    News  December 31, 2017   \n",
       " Drunk Bragging Trump Staffer Started Russian C...    News  December 31, 2017   \n",
       " Sheriff David Clarke Becomes An Internet Joke ...    News  December 30, 2017   \n",
       " Trump Is So Obsessed He Even Has Obama’s Name ...    News  December 29, 2017   \n",
       " Pope Francis Just Called Out Donald Trump Duri...    News  December 25, 2017   \n",
       "\n",
       "                                                                               conteúdo noticia Limpo  \n",
       "title                                                                                                  \n",
       " Donald Trump Sends Out Embarrassing New Year’s...  donald trump wish americans happy new year lea...  \n",
       " Drunk Bragging Trump Staffer Started Russian C...  house intelligence committee chairman devin nu...  \n",
       " Sheriff David Clarke Becomes An Internet Joke ...  friday reveal former milwaukee sheriff david c...  \n",
       " Trump Is So Obsessed He Even Has Obama’s Name ...  christmas day donald trump announce would back...  \n",
       " Pope Francis Just Called Out Donald Trump Duri...  pope francis use annual christmas day message ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news['conteúdo noticia Limpo'] = true_news['text'].apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
